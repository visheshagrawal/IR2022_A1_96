{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visheshagrawal/IR2022_A1_96/blob/main/IR_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIiUoYr-4dIj"
      },
      "source": [
        "---\n",
        "#Loading the colab essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc2_DpquxQ0c"
      },
      "source": [
        "## Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tD8sFDHN4SE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfeb22b-4b40-4048-e29c-bcce37c815fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Loading the colab essentials\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw0dLLQXs5BT"
      },
      "source": [
        "## Loading the Target Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFtSl-vHoYT8"
      },
      "source": [
        "### Aditi Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OCQMczDgoa5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ca248b-a303-435c-e79a-51fdb7a97e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/15TiqBLyk8RJBhmc1guzZPOx9pU0Ch0f6/IR/HW1/Dataset\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/IR/HW1/Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6dW0OS3jRZw"
      },
      "source": [
        "##Installing some library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOJiP1IMs8SS"
      },
      "source": [
        "##Loading the Important Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k01P7VXBs4Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe887a11-941f-4f95-c587-3fd85d550540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "# For missing values visualization\n",
        "import missingno as msno\n",
        "# For Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# For printing outputs in a beautiful way\n",
        "from pprint import pprint\n",
        "# For Normalization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# For confusion matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "# For cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# For F_beta score\n",
        "from sklearn.metrics import fbeta_score\n",
        "# Classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn import neighbors\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# For Model Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "# For splitting the Data into test and train\n",
        "from sklearn.model_selection import train_test_split\n",
        "# For F-beta\n",
        "from sklearn.metrics import fbeta_score, make_scorer\n",
        "# for legends\n",
        "import matplotlib.patches as mpatches\n",
        "#os annd list dir\n",
        "import os\n",
        "#json file processsing\n",
        "import json\n",
        "#NLP processing\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "\n",
        "# Download relevant data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eagA6tdVRtWn"
      },
      "source": [
        "## MatplotLib Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96kbNqUcI_Qh"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.style.use('seaborn')\n",
        "sns.set(style=\"whitegrid\", color_codes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwH9E3F1NfdY"
      },
      "outputs": [],
      "source": [
        "SMALL_SIZE = 14\n",
        "MEDIUM_SIZE = 16\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "#plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "#plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOcF0cpDoYxt"
      },
      "source": [
        "#main functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAq4a31coVjN"
      },
      "source": [
        "##traversing through files and saving the paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bj5BK6MFmESi"
      },
      "outputs": [],
      "source": [
        "files_list = os.listdir(\"Humor,Hist,Media,Food\")\n",
        "fpath=[]\n",
        "count=0\n",
        "for file in files_list:\n",
        "    count += 1\n",
        "    path2 = os.path.join(\"Humor,Hist,Media,Food\", file)\n",
        "    fpath.append(path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5jGFIHcmGbJ"
      },
      "outputs": [],
      "source": [
        "json.dump(fpath, open(\"file_path_mapping1.json\", \"w\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdg-M3G5nMSo",
        "outputId": "1a132e7d-7ce0-4026-9743-35c5346be17a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1133"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bcPrZyet_6C"
      },
      "source": [
        "##preprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n"
      ],
      "metadata": {
        "id": "vUrVpoQKL5h_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kKvSG33PxsRF"
      },
      "outputs": [],
      "source": [
        "def remove_special_char(text):\n",
        "  punct_char = string.punctuation\n",
        "  new_string=''\n",
        "  for char in text:\n",
        "    if char.isalnum() and not char.isdigit():\n",
        "      if char not in punct_char:\n",
        "        new_string+=char\n",
        "  return new_string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nPSh5xEoYYx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocessing(text):\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "\n",
        "  #Convert to lowercase\n",
        "  # Stop words removed\n",
        "\n",
        "  text = text.lower()\n",
        "  #Tokenize\n",
        "  text_tokens = word_tokenize(text)\n",
        "\n",
        "  #stemming\n",
        "  stemmer=PorterStemmer()\n",
        "  #text_tokens = list([stemmer.stem(word) for word in text_tokens])\n",
        "  #lemme\n",
        "  lemma=WordNetLemmatizer()\n",
        "  text_tokens = list([lemma.lemmatize(word) for word in text_tokens])\n",
        "  tokens_wo_stopwords = []\n",
        "  for token in text_tokens:\n",
        "    if token not in stopWords:\n",
        "      tokens_wo_stopwords.append(token)\n",
        "  #removing punctuation marks\n",
        "  tokens_wo_stopwords_punct = []\n",
        "  for x in tokens_wo_stopwords:\n",
        "    tokens_wo_stopwords_punct.append(remove_special_char(x))   \n",
        "  #Removing blank space tokens\n",
        "  validTokens = []\n",
        "  for x in tokens_wo_stopwords_punct:\n",
        "    if len(x)>1:\n",
        "      validTokens.append(x)\n",
        "  return validTokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aMvinbK3a3I"
      },
      "outputs": [],
      "source": [
        "def read_file(filePath):\n",
        "    try:\n",
        "        file = open(filePath, encoding=\"utf8\")\n",
        "        read = file.read().replace('\\n', ' ')    \n",
        "    except Exception as e:\n",
        "        file = open(filePath, encoding=\"unicode_escape\")\n",
        "        read = file.read().replace('\\n', ' ')\n",
        "    file.close()\n",
        "    return read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FWsvqKDuBdA"
      },
      "source": [
        "##inverted Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XbUuLHwgMTC"
      },
      "source": [
        "#running the code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YTrSfMarm6Fz"
      },
      "outputs": [],
      "source": [
        "dataset_output={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePLezvL2z2Td"
      },
      "outputs": [],
      "source": [
        "\n",
        "def indexing(tokens,file_num):\n",
        "  for i in tokens:\n",
        "    if i  not in dataset_output:\n",
        "      dataset_output[i]=[file_num]\n",
        "    else:\n",
        "      dataset_output[i].append(file_num)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05DEOP3r4wMz"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i,filePath in enumerate(fpath):\n",
        "  read=read_file(filePath)\n",
        "  tokens=preprocessing(read) #tokenizing and preprocessing\n",
        "  indexing(tokens,i) # performing indexin\n",
        "  json.dump(dataset_output, open(\"output_lemma_final_1.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgn2cH42ghtz",
        "outputId": "6a619315-1116-4e4a-e8b2-bbba88f09a26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54855"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "len(dataset_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3gQgIoFAE5Y",
        "outputId": "5f4fa870-901c-477c-9024-e1be47f728a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54855"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "len(dataset_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8X6luk-but1"
      },
      "source": [
        "#Queries handling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNh0T5GKb1NT"
      },
      "source": [
        "##Preprocessing input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "uZbzpXTJuTfz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9UGVbuN2bxdT"
      },
      "outputs": [],
      "source": [
        "def pre_process_q(inp):\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "  inp = inp.lower()\n",
        "  text_tokens = word_tokenize(inp)\n",
        "  #lemma  \n",
        "  lemma=WordNetLemmatizer()\n",
        "  text_tokens = list([lemma.lemmatize(word) for word in text_tokens])\n",
        "  \n",
        "  tokens_wo_stopwords = []\n",
        "  for token in text_tokens:\n",
        "    if token not in stopWords:\n",
        "      tokens_wo_stopwords.append(token)\n",
        "  #removing punctuation marks\n",
        "  tokens_wo_stopwords_punct = []\n",
        "  for x in tokens_wo_stopwords:\n",
        "    tokens_wo_stopwords_punct.append(remove_special_char(x))   \n",
        "  #Removing blank space tokens\n",
        "  validTokens = []\n",
        "  for x in tokens_wo_stopwords_punct:\n",
        "    if len(x)>1:\n",
        "      validTokens.append(x)\n",
        "  return validTokens\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_char(text):\n",
        "  punct_char = string.punctuation\n",
        "  new_string=''\n",
        "  for char in text:\n",
        "    if char.isalnum() and not char.isdigit():\n",
        "      if char not in punct_char:\n",
        "        new_string+=char\n",
        "  return new_string\n"
      ],
      "metadata": {
        "id": "Yyk-vIf8u6kP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6a9EFmjhfvz"
      },
      "source": [
        "##Query Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NqfhlohtsUYh"
      },
      "outputs": [],
      "source": [
        "input_sen=\"\"\n",
        "ops=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z7DOyDuvilsI"
      },
      "outputs": [],
      "source": [
        "def query_processing(input_sen,ops):\n",
        "  tokens1=pre_process_q(input_sen,ops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcQ6sTQAhikZ"
      },
      "source": [
        "###x OR y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YC2voZW2ilF5"
      },
      "outputs": [],
      "source": [
        "def x_or_y(term1_post,term2_post):\n",
        "  term1_postings=set(term1_post)\n",
        "  term2_postings=set(term2_post)\n",
        "  or_=set.union(term1_post,term2_post)\n",
        "  return or_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Vj5nn3hrin"
      },
      "source": [
        "###x AND y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6tad9PpA_zn3"
      },
      "outputs": [],
      "source": [
        "def x_and_y(term1_postings,term2_postings):\n",
        "  term1_postings=set(term1_postings)\n",
        "  term2_postings=set(term2_postings)\n",
        "  and_=set.intersection(term1_postings,term2_postings)\n",
        "  return and_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQdcet2GhuGT"
      },
      "source": [
        "###x AND NOT y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ghvR5ociAH3Y"
      },
      "outputs": [],
      "source": [
        "def f1(t1,t2):\n",
        "  candidate=[]\n",
        "  for i in t1:\n",
        "    if i not in t2:\n",
        "      candidate.append(i)\n",
        "  return candidate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hKdR3iEhxgh"
      },
      "source": [
        "###x OR NOT y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BzrKmqpHhdmm"
      },
      "outputs": [],
      "source": [
        "def f2(t1,t2):\n",
        "  #or not\n",
        "  candidate=[]\n",
        "  for i in range(0,1133):\n",
        "    if i not in t2:\n",
        "      candidate.append(i)\n",
        "  for j in t2:\n",
        "    if j not in t1:\n",
        "      candidate.append(i)\n",
        "  return candidate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwOecX1BqEhd"
      },
      "source": [
        "### Number of Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mHB4lGHkqTvr"
      },
      "outputs": [],
      "source": [
        "## merge algo\n",
        "def num_com(term1_postings, term2_postings):\n",
        "  i=0 #term1\n",
        "  j=0 # term2\n",
        "  result=0 # number of comparisions\n",
        "  term1_postings=list(term1_postings)\n",
        "  term2_postings=list(term2_postings)\n",
        "  term1_postings.sort()\n",
        "  term2_postings.sort()\n",
        "  while(i< len(term1_postings) and j<len(term2_postings)):\n",
        "    result+=1\n",
        "    if (term1_postings[i]>term2_postings[j]):\n",
        "      j+=1\n",
        "    elif (term1_postings[i]<term2_postings[j]):\n",
        "      i+=1\n",
        "    else:\n",
        "      j+=1\n",
        "      i+=1\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZv7fnPrK-gK"
      },
      "source": [
        "####Case 1 : AND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0x6PnewCkFp8"
      },
      "outputs": [],
      "source": [
        "def num_com_and(term1_postings,term2_postings):\n",
        "  a=num_com(term1_postings, term2_postings)\n",
        "  return a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT_6dTL8LKm_"
      },
      "source": [
        "####Case 2 : OR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DloJZLYVkI6r"
      },
      "outputs": [],
      "source": [
        "def num_com_or(term1_postings,term2_postings):\n",
        "  a=num_com(term1_postings, term2_postings)\n",
        "  return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_22btmQeLKvF"
      },
      "source": [
        "####Case 2 : AND NOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Aa5o6aE-kJ_w"
      },
      "outputs": [],
      "source": [
        "def num_com_andNOT(term1_postings,term2_postings):\n",
        "  universal_set=set(np.arange(1133))#edit number here\n",
        "  set_diff=universal_set-set(term2_postings)\n",
        "  a= num_com(term1_postings,set_diff)\n",
        "  return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7T14WtaLK1w"
      },
      "source": [
        "####Case 1 : ORNOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N3BGaMUMkLKD"
      },
      "outputs": [],
      "source": [
        "def num_com_orNOT(term1_postings,term2_postings):\n",
        "  universal_set=set(np.arange(1133))#edit number here\n",
        "  set_diff=universal_set-set(term2_postings)\n",
        "  a= num_com(term1_postings,set_diff)\n",
        "  return a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiGWAHyxK7-l"
      },
      "source": [
        "###getting a properly formatted string queryy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z3PNxiklLBEZ"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "def get_query(out_,inp,operands,pre_terms):\n",
        "  #out_ is the indexing file\n",
        "  #pre_terms is the pre processing term\n",
        "\n",
        "  comparisons=0\n",
        "\n",
        "  result=set(out_[pre_terms[0]])\n",
        "  n=len(pre_terms)\n",
        "  for i in range(1,n):\n",
        "    op=operands[i-1]\n",
        "    temp=out_[pre_terms[i]]\n",
        "    if(op=='AND'):\n",
        "      result=x_and_y(result,temp)\n",
        "      comparisons+=num_com_and(result,temp)\n",
        "    elif(op=='OR'):\n",
        "      result=x_or_y(result,temp)\n",
        "      comparisons+=num_com_or(result,temp)\n",
        "    elif(op=='AND NOT'):\n",
        "      result=f1(result,temp)#update\n",
        "      comparisons+=num_com_andNOT(result,temp)\n",
        "    elif(op=='OR NOT'):\n",
        "      result=f2(result,temp)#up\n",
        "      comparisons+=num_com_orNOT(result,temp)\n",
        "    else:\n",
        "      print(\"operand is invalid, please try again!\")\n",
        "      return -1\n",
        "  print(\"docs matched\",len(result))\n",
        "  print(\"number of comparisons\",comparisons)\n",
        "  mapping=json.load(open('file_path_mapping1.json'))\n",
        "  data= list(map(lambda i:(i,mapping[i]),result)) # list of tuples consisting of document id and their location\n",
        "  data.sort()\n",
        "  print(tabulate(data,headers=['Document ID','Location']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rPFXe58byA-3"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def main_():\n",
        "  n=int(input())\n",
        "  for i in range(n):\n",
        "    sentence=input(\"enter the sentence: \")\n",
        "    operands_=list(map(str.strip,input(\"Enter the operands:- \").upper().split(\",\")))\n",
        "    out_=json.load(open(\"output_lemma_final_1.json\")) #out_ is the indexing file\n",
        "    out_=defaultdict(lambda:[],out_)\n",
        "    #print(len(out_['kid']))\n",
        "    for item in out_:\n",
        "      out_[item]=list(set(out_[item]))\n",
        "    terms=pre_process_q(sentence)\n",
        "    #print(len(out_['kid']))\n",
        "    print(\"terms\",terms)\n",
        "    #print(\"out_\",out_.keys())\n",
        "    get_query(out_,sentence,operands_,terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv3aO_zA0UO3",
        "outputId": "3a5c87d4-41a9-4016-e962-c6c11ce093a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "enter the sentence: the lion stood proudly for a moment\n",
            "Enter the operands:- OR, OR, OR\n",
            "terms ['lion', 'stood', 'proudly', 'moment']\n",
            "docs matched 218\n",
            "number of comparisons 418\n",
            "  Document ID  Location\n",
            "-------------  ------------------------------------------\n",
            "            8  Humor,Hist,Media,Food/conan.txt\n",
            "           15  Humor,Hist,Media,Food/coyote.txt\n",
            "           17  Humor,Hist,Media,Food/incarhel.hum\n",
            "           22  Humor,Hist,Media,Food/insanity.hum\n",
            "           25  Humor,Hist,Media,Food/cartoon_.txt\n",
            "           29  Humor,Hist,Media,Food/ivan.hum\n",
            "           43  Humor,Hist,Media,Food/lbinter.hum\n",
            "           45  Humor,Hist,Media,Food/murphys.txt\n",
            "           49  Humor,Hist,Media,Food/lif&love.hum\n",
            "           50  Humor,Hist,Media,Food/lifeimag.hum\n",
            "           52  Humor,Hist,Media,Food/llong.hum\n",
            "           58  Humor,Hist,Media,Food/luggage.hum\n",
            "           62  Humor,Hist,Media,Food/lozerzon.hum\n",
            "           70  Humor,Hist,Media,Food/m0dzmen.hum\n",
            "           71  Humor,Hist,Media,Food/maecenas.hum\n",
            "           72  Humor,Hist,Media,Food/mailfrag.hum\n",
            "           73  Humor,Hist,Media,Food/mash.hum\n",
            "           74  Humor,Hist,Media,Food/meinkamp.hum\n",
            "           81  Humor,Hist,Media,Food/reasons.txt\n",
            "           90  Humor,Hist,Media,Food/montpyth.hum\n",
            "           93  Humor,Hist,Media,Food/news.hum\n",
            "           98  Humor,Hist,Media,Food/myheart.hum\n",
            "          105  Humor,Hist,Media,Food/cartoon.law\n",
            "          107  Humor,Hist,Media,Food/oldeng.hum\n",
            "          111  Humor,Hist,Media,Food/onetoone.hum\n",
            "          114  Humor,Hist,Media,Food/onetotwo.hum\n",
            "          115  Humor,Hist,Media,Food/deep.txt\n",
            "          119  Humor,Hist,Media,Food/pizzawho.hum\n",
            "          122  Humor,Hist,Media,Food/passage.hum\n",
            "          123  Humor,Hist,Media,Food/phorse.hum\n",
            "          125  Humor,Hist,Media,Food/peatchp.hum\n",
            "          130  Humor,Hist,Media,Food/popmusi.hum\n",
            "          132  Humor,Hist,Media,Food/policpig.hum\n",
            "          134  Humor,Hist,Media,Food/pro-fact.hum\n",
            "          135  Humor,Hist,Media,Food/quest.hum\n",
            "          141  Humor,Hist,Media,Food/fuckyou2.txt\n",
            "          144  Humor,Hist,Media,Food/radiolaf.hum\n",
            "          157  Humor,Hist,Media,Food/drunk.txt\n",
            "          163  Humor,Hist,Media,Food/smurfkil.hum\n",
            "          165  Humor,Hist,Media,Food/shuttleb.hum\n",
            "          170  Humor,Hist,Media,Food/socecon.hum\n",
            "          175  Humor,Hist,Media,Food/soleleer.hum\n",
            "          178  Humor,Hist,Media,Food/stone.hum\n",
            "          179  Humor,Hist,Media,Food/top10.txt\n",
            "          185  Humor,Hist,Media,Food/terrmcd'.hum\n",
            "          192  Humor,Hist,Media,Food/tfepisod.hum\n",
            "          193  Humor,Hist,Media,Food/timetr.hum\n",
            "          198  Humor,Hist,Media,Food/terrnieg.hum\n",
            "          202  Humor,Hist,Media,Food/let.go\n",
            "          203  Humor,Hist,Media,Food/lawyer.jok\n",
            "          204  Humor,Hist,Media,Food/hecomes.jok\n",
            "          209  Humor,Hist,Media,Food/cartoon.laws\n",
            "          215  Humor,Hist,Media,Food/whoops.hum\n",
            "          216  Humor,Hist,Media,Food/wagon.hum\n",
            "          219  Humor,Hist,Media,Food/wedding.hum\n",
            "          226  Humor,Hist,Media,Food/worldend.hum\n",
            "          245  Humor,Hist,Media,Food/strine.txt\n",
            "          248  Humor,Hist,Media,Food/lion.jok\n",
            "          249  Humor,Hist,Media,Food/marriage.hum\n",
            "          251  Humor,Hist,Media,Food/misc.1\n",
            "          252  Humor,Hist,Media,Food/wimptest.txt\n",
            "          261  Humor,Hist,Media,Food/xibovac.txt\n",
            "          262  Humor,Hist,Media,Food/childhoo.jok\n",
            "          266  Humor,Hist,Media,Food/commutin.jok\n",
            "          268  Humor,Hist,Media,Food/nigel.10\n",
            "          269  Humor,Hist,Media,Food/nigel.2\n",
            "          273  Humor,Hist,Media,Food/nigel.3\n",
            "          277  Humor,Hist,Media,Food/exam.50\n",
            "          278  Humor,Hist,Media,Food/idr2.txt\n",
            "          281  Humor,Hist,Media,Food/fascist.txt\n",
            "          286  Humor,Hist,Media,Food/nigel.5\n",
            "          296  Humor,Hist,Media,Food/passenge.sim\n",
            "          302  Humor,Hist,Media,Food/gas.txt\n",
            "          309  Humor,Hist,Media,Food/pepsideg.txt\n",
            "          319  Humor,Hist,Media,Food/psych_pr.quo\n",
            "          322  Humor,Hist,Media,Food/pukeprom.jok\n",
            "          329  Humor,Hist,Media,Food/puzzles.jok\n",
            "          330  Humor,Hist,Media,Food/quux_p.oem\n",
            "          336  Humor,Hist,Media,Food/eskimo.nel\n",
            "          340  Humor,Hist,Media,Food/prac1.jok\n",
            "          348  Humor,Hist,Media,Food/prac3.jok\n",
            "          362  Humor,Hist,Media,Food/psycho.txt\n",
            "          363  Humor,Hist,Media,Food/jc-elvis.inf\n",
            "          374  Humor,Hist,Media,Food/letgosh.txt\n",
            "          377  Humor,Hist,Media,Food/prac4.jok\n",
            "          379  Humor,Hist,Media,Food/luvstory.txt\n",
            "          380  Humor,Hist,Media,Food/lions.cat\n",
            "          382  Humor,Hist,Media,Food/oxymoron.jok\n",
            "          416  Humor,Hist,Media,Food/progrs.gph\n",
            "          420  Humor,Hist,Media,Food/tnd.1\n",
            "          421  Humor,Hist,Media,Food/quotes.txt\n",
            "          427  Humor,Hist,Media,Food/bw-phwan.hat\n",
            "          437  Humor,Hist,Media,Food/ambrose.bie\n",
            "          443  Humor,Hist,Media,Food/anim_lif.txt\n",
            "          454  Humor,Hist,Media,Food/various.txt\n",
            "          455  Humor,Hist,Media,Food/epitaph\n",
            "          461  Humor,Hist,Media,Food/vonthomp\n",
            "          462  Humor,Hist,Media,Food/quack26.txt\n",
            "          465  Humor,Hist,Media,Food/gown.txt\n",
            "          466  Humor,Hist,Media,Food/cogdis.txt\n",
            "          473  Humor,Hist,Media,Food/snapple.rum\n",
            "          492  Humor,Hist,Media,Food/candy.txt\n",
            "          498  Humor,Hist,Media,Food/pepper.txt\n",
            "          500  Humor,Hist,Media,Food/drinks.gui\n",
            "          507  Humor,Hist,Media,Food/boneles2.txt\n",
            "          508  Humor,Hist,Media,Food/dthought.txt\n",
            "          510  Humor,Hist,Media,Food/tpquotes.txt\n",
            "          532  Humor,Hist,Media,Food/kanalx.txt\n",
            "          539  Humor,Hist,Media,Food/barney.txt\n",
            "          542  Humor,Hist,Media,Food/bmdn01.txt\n",
            "          549  Humor,Hist,Media,Food/top10st1.txt\n",
            "          550  Humor,Hist,Media,Food/top10st2.txt\n",
            "          552  Humor,Hist,Media,Food/mundane.v2\n",
            "          553  Humor,Hist,Media,Food/japantv.txt\n",
            "          558  Humor,Hist,Media,Food/sw_err.txt\n",
            "          570  Humor,Hist,Media,Food/jason.fun\n",
            "          571  Humor,Hist,Media,Food/econridl.fun\n",
            "          572  Humor,Hist,Media,Food/oliver.txt\n",
            "          573  Humor,Hist,Media,Food/oliver02.txt\n",
            "          581  Humor,Hist,Media,Food/classicm.hum\n",
            "          582  Humor,Hist,Media,Food/cmu.share\n",
            "          589  Humor,Hist,Media,Food/bbh_intv.txt\n",
            "          592  Humor,Hist,Media,Food/dining.out\n",
            "          594  Humor,Hist,Media,Food/thievco.txt\n",
            "          595  Humor,Hist,Media,Food/aeonint.txt\n",
            "          596  Humor,Hist,Media,Food/mindvox\n",
            "          600  Humor,Hist,Media,Food/lion.txt\n",
            "          605  Humor,Hist,Media,Food/flux_fix.txt\n",
            "          616  Humor,Hist,Media,Food/consp.txt\n",
            "          624  Humor,Hist,Media,Food/booze1.fun\n",
            "          627  Humor,Hist,Media,Food/golnar.txt\n",
            "          633  Humor,Hist,Media,Food/rns_ency.txt\n",
            "          637  Humor,Hist,Media,Food/stuf10.txt\n",
            "          638  Humor,Hist,Media,Food/stuf11.txt\n",
            "          640  Humor,Hist,Media,Food/sfmovie.txt\n",
            "          644  Humor,Hist,Media,Food/iremember\n",
            "          647  Humor,Hist,Media,Food/msorrow\n",
            "          648  Humor,Hist,Media,Food/nameisreo.txt\n",
            "          655  Humor,Hist,Media,Food/epi_tton.txt\n",
            "          657  Humor,Hist,Media,Food/episimp2.txt\n",
            "          658  Humor,Hist,Media,Food/epi_.txt\n",
            "          665  Humor,Hist,Media,Food/scratchy.txt\n",
            "          672  Humor,Hist,Media,Food/gd_ql.txt\n",
            "          678  Humor,Hist,Media,Food/a_tv_t-p.com\n",
            "          682  Humor,Hist,Media,Food/allusion\n",
            "          685  Humor,Hist,Media,Food/anime.lif\n",
            "          689  Humor,Hist,Media,Food/wacky.ani\n",
            "          692  Humor,Hist,Media,Food/christop.int\n",
            "          694  Humor,Hist,Media,Food/clancy.txt\n",
            "          698  Humor,Hist,Media,Food/facedeth.txt\n",
            "          700  Humor,Hist,Media,Food/nukewar.txt\n",
            "          705  Humor,Hist,Media,Food/petshop\n",
            "          706  Humor,Hist,Media,Food/prac2.jok\n",
            "          707  Humor,Hist,Media,Food/pracjoke.txt\n",
            "          709  Humor,Hist,Media,Food/cybrtrsh.txt\n",
            "          713  Humor,Hist,Media,Food/filmgoof.txt\n",
            "          717  Humor,Hist,Media,Food/is_story.txt\n",
            "          730  Humor,Hist,Media,Food/humor9.txt\n",
            "          734  Humor,Hist,Media,Food/cookie.1\n",
            "          739  Humor,Hist,Media,Food/minn.txt\n",
            "          744  Humor,Hist,Media,Food/english.txt\n",
            "          751  Humor,Hist,Media,Food/beauty.tm\n",
            "          755  Humor,Hist,Media,Food/dromes.txt\n",
            "          758  Humor,Hist,Media,Food/initials.rid\n",
            "          772  Humor,Hist,Media,Food/annoy.fascist\n",
            "          779  Humor,Hist,Media,Food/practica.txt\n",
            "          781  Humor,Hist,Media,Food/dead3.txt\n",
            "          782  Humor,Hist,Media,Food/dead4.txt\n",
            "          783  Humor,Hist,Media,Food/dead5.txt\n",
            "          786  Humor,Hist,Media,Food/pat.txt\n",
            "          794  Humor,Hist,Media,Food/homebrew.txt\n",
            "          798  Humor,Hist,Media,Food/three.txt\n",
            "          818  Humor,Hist,Media,Food/insult.lst\n",
            "          819  Humor,Hist,Media,Food/insults1.txt\n",
            "          829  Humor,Hist,Media,Food/hackmorality.txt\n",
            "          879  Humor,Hist,Media,Food/caesardr.sal\n",
            "          883  Humor,Hist,Media,Food/bitnet.txt\n",
            "          910  Humor,Hist,Media,Food/beesherb.txt\n",
            "          921  Humor,Hist,Media,Food/bw.txt\n",
            "          930  Humor,Hist,Media,Food/proudlyserve.txt\n",
            "          931  Humor,Hist,Media,Food/jayjay.txt\n",
            "          934  Humor,Hist,Media,Food/calculus.txt\n",
            "          944  Humor,Hist,Media,Food/collected_quotes.txt\n",
            "          957  Humor,Hist,Media,Food/valujet.txt\n",
            "          958  Humor,Hist,Media,Food/computer.txt\n",
            "          963  Humor,Hist,Media,Food/mcd.txt\n",
            "          967  Humor,Hist,Media,Food/indgrdn.txt\n",
            "          976  Humor,Hist,Media,Food/reeves.txt\n",
            "          977  Humor,Hist,Media,Food/moore.txt\n",
            "          978  Humor,Hist,Media,Food/chickenheadbbs.txt\n",
            "          980  Humor,Hist,Media,Food/ukunderg.txt\n",
            "          982  Humor,Hist,Media,Food/namaste.txt\n",
            "          984  Humor,Hist,Media,Food/lifeonledge.txt\n",
            "          996  Humor,Hist,Media,Food/grail.txt\n",
            "          998  Humor,Hist,Media,Food/mel.txt\n",
            "          999  Humor,Hist,Media,Food/hackingcracking.txt\n",
            "         1008  Humor,Hist,Media,Food/ghostfun.hum\n",
            "         1009  Humor,Hist,Media,Food/mlverb.hum\n",
            "         1013  Humor,Hist,Media,Food/throwawa.hum\n",
            "         1019  Humor,Hist,Media,Food/solders.hum\n",
            "         1023  Humor,Hist,Media,Food/kaboom.hum\n",
            "         1029  Humor,Hist,Media,Food/b-2.jok\n",
            "         1035  Humor,Hist,Media,Food/suicide2.txt\n",
            "         1036  Humor,Hist,Media,Food/nigel10.txt\n",
            "         1043  Humor,Hist,Media,Food/badday.hum\n",
            "         1061  Humor,Hist,Media,Food/cabbage.txt\n",
            "         1063  Humor,Hist,Media,Food/butwrong.hum\n",
            "         1066  Humor,Hist,Media,Food/nihgel_8.9\n",
            "         1070  Humor,Hist,Media,Food/macsfarm.old\n",
            "         1084  Humor,Hist,Media,Food/manners.txt\n",
            "         1093  Humor,Hist,Media,Food/cuchy.hum\n",
            "         1098  Humor,Hist,Media,Food/dingding.hum\n",
            "         1100  Humor,Hist,Media,Food/art-fart.hum\n",
            "         1104  Humor,Hist,Media,Food/doggun.sto\n",
            "         1107  Humor,Hist,Media,Food/murphy_l.txt\n",
            "         1108  Humor,Hist,Media,Food/engineer.hum\n",
            "         1117  Humor,Hist,Media,Food/moose.txt\n",
            "         1129  Humor,Hist,Media,Food/female.jok\n"
          ]
        }
      ],
      "source": [
        "main_()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaya3gv65mhj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "IR_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}